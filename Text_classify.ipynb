{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "\n",
    "pos_data = []\n",
    "neg_data = []\n",
    "\n",
    "# 读取数据\n",
    "for file_name in os.listdir('正面/'):\n",
    "    with open('正面/' + file_name, 'r', encoding='utf-8') as in_data:\n",
    "        line = in_data.readlines()[0].strip()\n",
    "        line = ' '.join(jieba.cut(line))\n",
    "        pos_data.append(line)\n",
    "\n",
    "for file_name in os.listdir('负面/'):\n",
    "    with open('负面/' + file_name, 'r', encoding='utf-8') as in_data:\n",
    "        line = in_data.readlines()[0].strip()\n",
    "        line = ' '.join(jieba.cut(line))\n",
    "        neg_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词\n",
    "stop_words = []\n",
    "with open('chinese_stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        stop_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除停用词\n",
    "pos_data_del_stopwords = []\n",
    "for item in pos_data:\n",
    "    words = [w for w in item.split() if w not in stop_words]\n",
    "    pos_data_del_stopwords.append(' '.join(words))\n",
    "    \n",
    "neg_data_del_stopwords = []\n",
    "for item in neg_data:\n",
    "    words = [w for w in item.split() if w not in stop_words]\n",
    "    neg_data_del_stopwords.append(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "text_data = neg_data_del_stopwords + pos_data_del_stopwords\n",
    "\n",
    "vector = TfidfVectorizer()\n",
    "features = vector.fit_transform(text_data).toarray()\n",
    "\n",
    "labels = np.vstack((np.zeros((len(neg_data_del_stopwords), 1)), np.ones((len(pos_data_del_stopwords), 1))))\n",
    "\n",
    "data = np.hstack((features, labels))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=2)\n",
    "\n",
    "lr = LogisticRegression().fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-4fc969694826>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 建立word与向量的索引\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT_DATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Input,Flatten\n",
    "from keras.layers import Conv1D,MaxPooling1D,Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "TEXT_DATA_DIR='glove.6B.100d.txt'\n",
    "MAX_SEQUENCE_LENGTH=1000\n",
    "MAX_NB_WORDS=10000\n",
    "EMBEDDING_DIM=100\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "# 建立word与向量的索引\n",
    "embeddings_index={}\n",
    "f=open(TEXT_DATA_DIR,'r',encoding='utf-8')\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# 选定的类别\n",
    "selected_categories = [\n",
    "    'comp.graphics',\n",
    "    'rec.motorcycles',\n",
    "    'rec.sport.baseball',\n",
    "    'misc.forsale',\n",
    "    'sci.electronics',\n",
    "    'sci.med',\n",
    "    'talk.politics.guns',\n",
    "    'talk.religion.misc']\n",
    "\n",
    "# 加载数据\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      categories=selected_categories,\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     categories=selected_categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "texts = newsgroups_train['data']\n",
    "labels = newsgroups_train['target']\n",
    "print(len(texts))\n",
    "print(np.unique(labels))\n",
    "print(labels)\n",
    "texts = [t for t in texts]\n",
    "print(type(texts[0]),texts)\n",
    "\n",
    "# 文本向量化\n",
    "tokenizer=Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_NB_WORDS)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Data validation split.')\n",
    "\n",
    "# 将data划分为训练集和验证集\n",
    "indices=np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "labels=labels[indices]\n",
    "num_validation_samples=int(VALIDATION_SPLIT*data.shape[0])\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "\n",
    "# 创建词嵌入\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # 如果单词没有在word index,全部设置为0.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_NB_WORDS,\n",
    "                            trainable=False)\n",
    "\n",
    "print(\"正在初始化模型...\")\n",
    "\n",
    "# 初始化卷积层和池化层\n",
    "sequence_input=Input(shape=(MAX_NB_WORDS,),dtype='int32')\n",
    "embedded_sequences=embedding_layer(sequence_input)\n",
    "x=Conv1D(128,5,activation='relu')(embedded_sequences)\n",
    "x=MaxPooling1D(5)(x)\n",
    "x=Conv1D(128,5,activation='relu')(x)\n",
    "x=MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(8, activation='softmax')(x)\n",
    "\n",
    "model=Model(sequence_input,preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "print(\"正在训练模型...\")\n",
    "# 训练模型\n",
    "model.fit(x_train,y_train,\n",
    "          batch_size=128,\n",
    "          nb_epoch=10,\n",
    "          validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 5, activation=\"relu\", padding=\"valid\")`\n",
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=4)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compiled\n",
      "4484\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[3 3 1 ... 3 0 4]\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_preprocessing\\text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48098 unique tokens.\n",
      "Shape of data tensor: (4484, 1000)\n",
      "Shape of label tensor: (4484,)\n",
      "Initialize model.\n",
      "Build model...\n",
      "Model Compiled\n",
      "x_train shape: (3588, 1000)\n",
      "x_test shape: (896, 1000)\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:189: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3588 samples, validate on 896 samples\n",
      "Epoch 1/2\n",
      "3588/3588 [==============================] - 99s 27ms/step - loss: -28.4770 - acc: 0.1318 - val_loss: -37.1159 - val_acc: 0.1339\n",
      "Epoch 2/2\n",
      "3588/3588 [==============================] - 95s 26ms/step - loss: -36.7679 - acc: 0.1318 - val_loss: -37.1159 - val_acc: 0.1339\n",
      "896/896 [==============================] - 5s 6ms/step\n",
      "Test score: -37.1158660863127\n",
      "Test accuracy: 0.13392857501250027\n",
      "Training\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.196209587513934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "class CNNLSTM():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # Embedding\n",
    "    max_features = 10000\n",
    "    maxlen = 1000\n",
    "    embedding_size = 128\n",
    "\n",
    "    # Convolution\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "\n",
    "    # LSTM\n",
    "    lstm_output_size = 70\n",
    "\n",
    "    # Training\n",
    "    batch_size = 30\n",
    "    epochs = 2\n",
    "\n",
    "    '''\n",
    "    Note:\n",
    "    batch_size is highly sensitive.\n",
    "    Only 2 epochs are needed as the dataset is very small.\n",
    "    '''\n",
    "    def initialize(self):\n",
    "        print('Build model...')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.max_features, self.embedding_size, input_length=self.maxlen))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv1D(self.filters,\n",
    "                         self.kernel_size,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_length=self.pool_size))\n",
    "        model.add(LSTM(self.lstm_output_size))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        print('Model Compiled')\n",
    "        return model\n",
    "\n",
    "    def train(self, model):\n",
    "        print('Loading data...')\n",
    "        (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=self.max_features)\n",
    "        print(len(x_train), 'train sequences')\n",
    "        print(len(x_test), 'test sequences')\n",
    "\n",
    "        print('Pad sequences (samples x time)')\n",
    "        x_train = sequence.pad_sequences(x_train, maxlen=self.maxlen)\n",
    "        x_test = sequence.pad_sequences(x_test, maxlen=self.maxlen)\n",
    "        print('x_train shape:', x_train.shape)\n",
    "        print('x_test shape:', x_test.shape)\n",
    "\n",
    "\n",
    "        print('Train...')\n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=self.batch_size,\n",
    "                  epochs=self.epochs,\n",
    "                  validation_data=(x_test, y_test))\n",
    "        score, acc = model.evaluate(x_test, y_test, batch_size=self.batch_size)\n",
    "        print('Test score:', score)\n",
    "        print('Test accuracy:', acc)\n",
    "\n",
    "model = CNNLSTM().initialize()\n",
    "\n",
    "# ======\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from cnn_lstm import CNNLSTM\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "selected_categories = [\n",
    "    'comp.graphics',\n",
    "    'comp.windows.x',\n",
    "    'rec.motorcycles',\n",
    "    'rec.sport.baseball',\n",
    "    'sci.crypt',\n",
    "    'sci.med',\n",
    "    'talk.politics.guns',\n",
    "    'talk.religion.misc']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      categories=selected_categories,\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     categories=selected_categories,\n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "texts = newsgroups_train['data']\n",
    "labels = newsgroups_train['target']\n",
    "\n",
    "print(len(texts))\n",
    "print(np.unique(labels))\n",
    "print(labels)\n",
    "\n",
    "texts = [t for t in texts]\n",
    "print(type(texts[0]))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "ax_features = 10000\n",
    "maxlen = 1000\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "#\n",
    "print('Initialize model.')\n",
    "\n",
    "model = CNNLSTM().initialize()\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_val.shape)\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=epochs,\n",
    "          validation_data=(x_val, y_val), verbose=1)\n",
    "score, acc = model.evaluate(x_val, y_val, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5, verbose=1)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "print('Training')\n",
    "\n",
    "logreg.fit(x_train, y_train)\n",
    "joblib.dump(logreg, 'logreg.pkl')\n",
    "\n",
    "\n",
    "clf = joblib.load('logreg.pkl')\n",
    "\n",
    "pred = clf.predict(x_train)\n",
    "print(((pred == y_train).sum() * 100.0) / (np.shape(y_train)[0] * 1.0))\n",
    "\n",
    "\n",
    "# from sklearn import datasets\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb = GaussianNB()\n",
    "# gnb.fit(x_train, y_train)\n",
    "# joblib.dump(gnb, 'gnb.pkl')\n",
    "# clf = joblib.load('gnb.pkl')\n",
    "# pred = clf.predict(x_train)\n",
    "# print(((pred == y_train).sum() * 100.0) / (np.shape(y_train)[0] * 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理\n",
    "1、处理英文缩写\n",
    "\n",
    "2、转换为小写字母\n",
    "\n",
    "3、删除标点等非英文字符\n",
    "\n",
    "4、分词\n",
    "\n",
    "5、词性还原\n",
    "\n",
    "6、删除停用词\n",
    "\n",
    "7、拼写检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['student',\n",
       " 'do',\n",
       " 'not',\n",
       " 'know',\n",
       " 'will',\n",
       " 'not',\n",
       " 'lear',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'where',\n",
       " 'from',\n",
       " 'look',\n",
       " 'egg']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "t = ['a', 'am', 'i', 'are', 'you']  # 停用词\n",
    "\n",
    "s = \"I'm a student. I don't know. I willn't learing. 14 years ago? Where are you from? looking! eggs?\"\n",
    "\n",
    "s = s.replace(\"I'm\", 'I am').replace(\"don't\", 'do not').replace(\"willn't\", 'will not')  # 处理英文缩写\n",
    "\n",
    "words_sys = re.sub(\"[^a-zA-Z]\", \" \", s.lower())  # 删除标点等非英文字符，转换成小写字母\n",
    "\n",
    "words_sys = ' '.join(words_sys.split())\n",
    "\n",
    "words = word_tokenize(words_sys)  # 分词\n",
    "\n",
    "porter_stem = PorterStemmer()\n",
    "\n",
    "words = [porter_stem.stem(w) for w in words if w not in t]  # 词性还原，删除停用词\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
