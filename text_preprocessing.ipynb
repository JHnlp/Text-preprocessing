{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、英文缩写替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The story loses its bite in a last-minute happy ending that is even less plausible than the rest of the picture . It is funny .'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The story loses its bite in a last-minute happy ending that's even less plausible than the rest of the picture . It's funny .\"\n",
    "\n",
    "text.replace(\"that's\", \"that is\").replace(\"It's\", \"It is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、大写字母转换为小写字母"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the story loses its bite in a last-minute happy ending that is even less plausible than the rest of the picture . it is funny .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The story loses its bite in a last-minute happy ending that is even less plausible than the rest of the picture . It is funny .'\n",
    "\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、删除标点符号、数字及其它特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the story loses its bite in a last minute happy ending that is even less plausible than the rest of the picture it is funny'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = 'the story loses its bite in a 12 last-minute happy ending %@ that is even less plausible #$ than the rest of the picture . it is funny .'\n",
    "\n",
    "text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\" \".join(text.split())  # 删除多余的空格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用nltk库进行分词：\n",
      " ['the', 'story', 'loses', 'its', 'bite', 'in', 'a', 'last', 'minute', 'happy', 'ending', 'that', 'is', 'even', 'less', 'plausible', 'than', 'the', 'rest', 'of', 'the', 'picture', 'it', 'is', 'funny']\n",
      "使用split函数进行分词：\n",
      " ['the', 'story', 'loses', 'its', 'bite', 'in', 'a', 'last', 'minute', 'happy', 'ending', 'that', 'is', 'even', 'less', 'plausible', 'than', 'the', 'rest', 'of', 'the', 'picture', 'it', 'is', 'funny']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'the story loses its bite in a last minute happy ending that is even less plausible than the rest of the picture it is funny'\n",
    "\n",
    "print(\"使用nltk库进行分词：\\n\", word_tokenize(text))\n",
    "print(\"使用split函数进行分词：\\n\", text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n",
      "countri\n",
      "happi\n",
      "countri\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# 第一种方法\n",
    "stem_porter = PorterStemmer()\n",
    "print(stem_porter.stem('happy'))\n",
    "print(stem_porter.stem('country'))\n",
    "\n",
    "# 第二种方法\n",
    "snowball_stem = SnowballStemmer(\"english\") # 指定语言：english\n",
    "print(snowball_stem.stem('happy'))\n",
    "print(snowball_stem.stem('country'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('was', pos='v'))\n",
    "print(lemma.lemmatize('cats', pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7、删除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story loses bite last minute happy ending even less plausible rest picture funny\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "text = 'the story loses its bite in a last minute happy ending that is even less plausible than the rest of the picture it is funny'\n",
    "\n",
    "del_stopwords = [w for w in text.split() if w not in stop_words]\n",
    "\n",
    "print(\" \".join(del_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、删除标点符号、数字、字母及其它字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京时间月日赛季亚洲冠军联赛组将展开小组赛第六轮争夺中超两强北京国安和上海上港都面临生死大战好在两支队伍都掌握着晋级主动权只要取胜即可锁定小组出线权在国内赛场风生水起的北京国安渴望改写在亚冠联赛从未有过连续从小组中脱颖而出的历史伤兵满营的上海上港则希望延续只要参加亚冠比赛就必晋级淘汰赛阶段的纪录\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"$$北京时间5月21日，2019赛季亚洲冠军联赛G、H组将展开小组赛第六轮争夺。%&\\\n",
    "中超两强北京国安和上海上港都面临生死大战，好在两支队伍都掌握着晋级主动权，只要取胜即可锁定小组出线权。@#\\\n",
    "在国内赛场风生水起的北京国安，渴望改写在亚冠联赛从未有过连续从小组中脱颖而出的历史；\\\n",
    "伤兵满营的上海上港，则希望延续只要参加亚冠比赛就必晋级淘汰赛阶段的纪录。$$\"\n",
    "\n",
    "text = re.sub(\"[，。？‘’“”《》；：！——……（）【】、a-zA-Z0-9@#$%&*/\\,.?!\"\"''{}+<>~]\", \"\", text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['他', '毕业', '于', '北京大学', '计算机', '学院']\n",
      "['他', '毕业', '于', '北京', '北京大学', '大学', '计算', '计算机', '算机', '学院']\n",
      "['他', '毕业', '于', '北京', '大学', '北京大学', '计算', '算机', '计算机', '学院']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"他毕业于北京大学计算机学院\"\n",
    "\n",
    "# 精准模式\n",
    "print(list(jieba.cut(text, cut_all=False)))\n",
    "\n",
    "# 全模式\n",
    "print(list(jieba.cut(text, cut_all=True)))\n",
    "\n",
    "# 搜索引擎模式\n",
    "print(list(jieba.cut_for_search(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、删除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精准模式：\n",
      " ['北京', '时间', '月', '日', '赛季', '亚洲', '冠军联赛', '组将', '展开', '小组赛', '第六轮', '争夺', '中超', '两强', '北京国安', '和', '上海', '上港', '都', '面临', '生死', '大战', '好', '在', '两支', '队伍', '都', '掌握', '着', '晋级', '主动权', '只要', '取胜', '即可', '锁定', '小组', '出线权', '在', '国内', '赛场', '风生水', '起', '的', '北京国安', '渴望', '改写', '在', '亚冠', '联赛', '从未有过', '连续', '从', '小组', '中', '脱颖而出', '的', '历史', '伤兵', '满营', '的', '上海', '上港', '则', '希望', '延续', '只要', '参加', '亚冠', '比赛', '就', '必', '晋级', '淘汰赛', '阶段', '的', '纪录']\n",
      "删除停用词：\n",
      " ['北京', '时间', '赛季', '亚洲', '冠军联赛', '组将', '展开', '小组赛', '第六轮', '争夺', '中超', '两强', '北京国安', '上海', '上港', '面临', '生死', '大战', '两支', '队伍', '掌握', '晋级', '主动权', '取胜', '即可', '锁定', '小组', '出线权', '国内', '赛场', '风生水', '起', '北京国安', '渴望', '改写', '亚冠', '联赛', '从未有过', '连续', '小组', '中', '脱颖而出', '历史', '伤兵', '满营', '上海', '上港', '希望', '延续', '参加', '亚冠', '比赛', '晋级', '淘汰赛', '阶段', '纪录']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"北京时间月日赛季亚洲冠军联赛组将展开小组赛第六轮争夺中超两强北京国安和上海上港都面临生死大战\\\n",
    "好在两支队伍都掌握着晋级主动权只要取胜即可锁定小组出线权在国内赛场风生水起的北京国安渴望改写在亚冠联赛从未有过连续从小组中脱颖而出的历史\\\n",
    "伤兵满营的上海上港则希望延续只要参加亚冠比赛就必晋级淘汰赛阶段的纪录\"\n",
    "\n",
    "stopwords = [\"月\", \"日\", \"的\", \"在\", \"只要\" , \"风生水起\", \"则\", \"好\", \"和\", \"都\", \"就\", \"必\", \"着\", \"从\"]  # 停用词\n",
    "\n",
    "text_cut = list(jieba.cut(text))  # 精准模式\n",
    "print(\"精准模式：\\n\", text_cut)\n",
    "\n",
    "del_stopwords = [w for w in text_cut if w not in stopwords]  # 删除停用词\n",
    "\n",
    "print(\"删除停用词：\\n\", del_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
