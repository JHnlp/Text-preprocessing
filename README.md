# Text-preprocessing
## 摘要
不管是中文文本还是英文文本，计算机都不能理解，我们需要把这些原始的东西转换成计算机可以理解的东西，计算机只能理解数值，所以，归根结底还是要把文本转换成计算机能理解的向量的形式。在进行文本分类或者聚类等任务之前都需要对文本进行预处理，中文文本和英文文本的预处理方式有所差别。本文主要介绍中英文文本的在文本预处理过程中的步骤。

## 1、英文文本预处理

（1）英文缩写替换

语料库中有些英文单词是以缩写的形式存在的，比如it's、that's、I'm等等，在预处理过程中需要把它们替换为it is、that is、I am等等。

（2）大写字母转换为小写字母

在语料库中，单词或者字母有大小写之分，比如like和Like，在统计单词的时候希望它们是相同的单词，所有需要把大写字母转换为小写字母。

（3）删除标点符号、数字及其它特殊字符，让文本只保留英文

标点符号、数字及其它特殊字符对文本分类不起作用，通常会把它们删除，以减少特征的维度。一般用正则化方法进行删除。

（4）分词

英文文本分词方法可以根据所提供的文本进行选择，如果文本中单词和标点符号或者其它字符是以空格隔开的，例如"a little of both ."，那么可以直接使用split()方法；如果文本中单词和标点符号没有用空格隔开，例如"a little of both."，可以使用nltk库中的word_tokenize()方法。nltk库安装也比较简单，在windows下，用pip install nltk进行安装即可。

（5）拼写检查

由于英文文本中可能有拼写错误，因此一般需要进行拼写检查。如果确信分析的文本没有拼写错误，可以略去此步。拼写检查，一般用pyenchant类库完成。

（6）词干提取和词形还原

词干提取(stemming)和词型还原(lemmatization)是英文文本预处理的特色。两者其实有共同点，即都是要找到词的原始形式。只不过词干提取(stemming)会更加激进一点，它在寻找词干的时候可以会得到不是词的词干。比如"imaging"的词干可能得到的是"imag", 并不是一个词。在nltk中，做词干提取的方法有PorterStemmer，LancasterStemmer和SnowballStemmer。推荐使用SnowballStemmer。这个类可以处理很多种语言，当然，除了中文。而词形还原则保守一些，它一般只对能够还原成一个正确的词的词进行处理。比如，is、are、was等词形还原成be，cats还原成cat。在nltk库中，词形还原的方法有WordNetLemmatizer。


（7）删除停用词

停用词对文本分类也没有什么影响，常见的停用词，比如：a、the、of等等。nltk库自带英文停用词，from nltk.corpus import stopwords，也可以添加新的停用词。

## 2、中文文本预处理

（1）删除标点符号、数字、字母及其他字符，让文本只保留汉字

（2）分词

jieba是目前最好的python中文分词库，它的分词模式分为：精准模式、全模式和搜索引擎模式。

（3）删除停用词

首先需要在网上下载中文的停用词表或者自己制作停用词表，也可以下载好停用词表，然后加上一些其它的停用词。

## 总结
以上便是整个中英文文本预处理的步骤，这些步骤可以解决大多数的文本处理任务。
